{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import re, collections, os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    \"\"\" This class is used to tokenize the input text and create a vocabulary of tokens. \"\"\"\n",
    "    def __init__(self, text, num_merges):\n",
    "        self.text = text\n",
    "        self.num_merges = num_merges\n",
    "\n",
    "    def initialize_vocabulary(self,text):\n",
    "        \"\"\" Initialize the vocabulary from the input text. \"\"\"\n",
    "        vocab = collections.defaultdict(int)\n",
    "        words = text.strip().split()\n",
    "        for word in words:\n",
    "            vocab[' '.join(list(word)) + ' '] += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_tokens_and_frequencies(self,vocab):\n",
    "        \"\"\" Get the tokens and how often they occur in the vocabulary. \"\"\"\n",
    "        tokens = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            word_tokens = word.split()\n",
    "            for token in word_tokens:\n",
    "                tokens[token] += freq\n",
    "        return tokens\n",
    "    \n",
    "    def get_pairs_and_counts(self,vocab):\n",
    "        \"\"\" Get the pairs of tokens and how often they occur in the vocabulary. \"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i],symbols[i+1]] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_pair_in_vocabulary(self,pair, vocab_in):\n",
    "        \"\"\" Merge the most frequent pair of tokens in the vocabulary. \"\"\"\n",
    "        vocab_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in vocab_in:\n",
    "            word_out = p.sub(''.join(pair), word)\n",
    "            vocab_out[word_out] = vocab_in[word]\n",
    "        return vocab_out\n",
    "    \n",
    "    def tokenize(self):\n",
    "        \"\"\" Tokenize the input text. \"\"\"\n",
    "        if os.path.exists('./data/tokens_{}merges.pt'.format(self.num_merges)):\n",
    "            print(\"Loading tokenized data from file\")\n",
    "            tokens = torch.load('./data/tokens_{}merges.pt'.format(self.num_merges),weights_only=False)\n",
    "            vocab = torch.load('./data/vocab_{}merges.pt'.format(self.num_merges),weights_only=False)\n",
    "            return tokens, vocab\n",
    "        \n",
    "        # Initialize the vocabulary from the input text\n",
    "        vocab = self.initialize_vocabulary(self.text)\n",
    "\n",
    "        # Merge the most frequent pair of tokens num_merges times\n",
    "        with tqdm.tqdm(range(self.num_merges), position=0, leave=True) as pbar:\n",
    "            for i in pbar:\n",
    "                tokens = self.get_tokens_and_frequencies(vocab)\n",
    "                pairs = self.get_pairs_and_counts(vocab)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                most_frequent_pair = max(pairs, key=pairs.get)\n",
    "                vocab = self.merge_pair_in_vocabulary(most_frequent_pair, vocab)\n",
    "                pbar.set_description(f'Num merges: {i + 1}')\n",
    "\n",
    "        # Find the tokens and how often they occur in the vocabulary one last time\n",
    "        tokens = self.get_tokens_and_frequencies(vocab)\n",
    "\n",
    "        tokens[' '] = self.text.count(' ')\n",
    "        tokens['\\n'] = self.text.count('\\n')\n",
    "\n",
    "        torch.save(tokens, './data/tokens_{}merges.pt'.format(self.num_merges))\n",
    "        torch.save(vocab, './data/vocab_{}merges.pt'.format(self.num_merges))\n",
    "\n",
    "        return tokens, vocab\n",
    "\n",
    "\n",
    "    def tokenize_text(self, tokens):\n",
    "        \"\"\" Tokenize the input text using the tokens. \"\"\"\n",
    "        # Load tokenized data if it exists\n",
    "        if os.path.exists('./data/tokenized_{}merges.pt'.format(self.num_merges)):\n",
    "            print(\"Loading tokenized data from file\")\n",
    "            tokenized_text = torch.load('./data/tokenized_{}merges.pt'.format(self.num_merges),weights_only=False)\n",
    "            return tokenized_text\n",
    "\n",
    "        token_list = list(tokens.keys())\n",
    "        tokenized_text = []  # List to store the tokenized text\n",
    "        i = 0  # Index to keep track of the current position in the text\n",
    "\n",
    "        print(\"Nombre de tokens trouvés :\", len(token_list))\n",
    "\n",
    "        with tqdm.tqdm(total=len(self.text), position=0, leave=True) as pbar:\n",
    "            pbar.set_description(\"Tokenizing text\")\n",
    "            while i < len(self.text):\n",
    "                match = None\n",
    "                # loop through the tokens to find the longest match\n",
    "                for token in sorted(token_list, key=len, reverse=True):\n",
    "                    if self.text[i:i+len(token)] == token: \n",
    "                        match = token\n",
    "                        break\n",
    "                    \n",
    "                if match:  \n",
    "                    tokenized_text.append(match)\n",
    "                    i += len(match)  # Move the index to the end of the token\n",
    "                    pbar.update(len(match))\n",
    "                else:\n",
    "                    print(\"Token non trouvé pour le texte restant :\", self.text[i:])\n",
    "                    break \n",
    "\n",
    "        # Save the tokenized text\n",
    "        torch.save(tokenized_text, './data/tokenized_{}merges.pt'.format(self.num_merges))\n",
    "        return tokenized_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('./data/Shakespeare.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Num merges: 1000: 100%|██████████| 1000/1000 [01:51<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de tokens trouvés : 1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing text: 100%|██████████| 1115394/1115394 [02:08<00:00, 8697.13it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(data, 1000)\n",
    "tokens, vocab = tokenizer.tokenize()\n",
    "tokenized_text = tokenizer.tokenize_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', ' ', 'Citizen:', '\\n', 'Be', 'fore', ' ', 'we', ' ', 'pro', 'ce', 'ed', ' ', 'any', ' ', 'f', 'ur', 'ther', ',', ' ', 'hear', ' ', 'me', ' ', 'speak', '.', '\\n', '\\n', 'All', ':', '\\n', 'S', 'pe', 'ak', ',', ' ', 'speak', '.', '\\n', '\\n', 'First', ' ', 'Citizen:', '\\n', 'You', ' ', 'are', ' ', 'all', ' ', 'res', 'ol', 'ved', ' ', 'ra', 'ther', ' ', 'to', ' ', 'die', ' ', 'than', ' ', 'to', ' ', 'fa', 'mis', 'h', '?', '\\n', '\\n', 'All', ':', '\\n', 'R', 'es', 'ol', 'ved', '.', ' ', 'res', 'ol', 'ved', '.', '\\n', '\\n', 'First', ' ', 'Citizen:', '\\n', 'First', ',', ' ', 'you', ' ', 'know', ' ', 'C', 'a', 'i']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# CharDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, tokens, block_size):\n",
    "\n",
    "        chars = list(tokens.keys())\n",
    "\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "        self.data = torch.tensor([self.stoi[ch] for ch in tokenized_text], dtype=torch.long)\n",
    "        \n",
    "        self.vocab_size = len(chars)\n",
    "        self.block_size = block_size\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        input_chunk = chunk[:-1]\n",
    "        target = chunk[1:]\n",
    "        return input_chunk, target\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def get_itos(self):\n",
    "        return self.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
